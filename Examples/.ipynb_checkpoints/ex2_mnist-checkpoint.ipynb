{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle, resample\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "import pickle\n",
    "import sys, os\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\n",
    "from miniflow import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_img_file = '../mnist_data/train-images-idx3-ubyte.gz'\n",
    "train_label_file = '../mnist_data/train-labels-idx1-ubyte.gz'\n",
    "test_img_file = '../mnist_data/t10k-images-idx3-ubyte.gz'\n",
    "test_label_file = '../mnist_data/t10k-labels-idx1-ubyte.gz'\n",
    "\n",
    "save_file = '../mnist_data/mnist.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_image(fname):\n",
    "    with gzip.open(fname, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "    data = data.reshape(-1, 784)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def read_label(fname):\n",
    "    with gzip.open(fname, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "        \n",
    "    return data\n",
    "\n",
    "def create_pkl(save_file, data):\n",
    "    if os.path.exists(save_file):\n",
    "        return\n",
    "    \n",
    "    with open(save_file, 'wb') as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist_data = {}\n",
    "mnist_data['train_img'] = read_image(train_img_file)\n",
    "mnist_data['train_label'] = read_label(train_label_file)\n",
    "mnist_data['test_img'] = read_image(test_img_file)\n",
    "mnist_data['test_label'] = read_label(test_label_file)\n",
    "\n",
    "create_pkl(save_file, mnist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open(save_file, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "X_train, y_train = data['train_img'], data['train_label']\n",
    "X_test, y_test = data['test_img'], data['test_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalized(data):\n",
    "    data = data.astype(np.float32)\n",
    "    return data / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalized data\n",
    "X_train = normalized(X_train)\n",
    "X_test = normalized(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(data, n):\n",
    "    one_hot_encoding = np.zeros([data.shape[-1], n])\n",
    "    one_hot_encoding[np.arange(data.shape[-1]), data] = 1 \n",
    "    \n",
    "    return one_hot_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y_train = one_hot(y_train, 10)\n",
    "#y_test = one_hot(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (54000, 784) (54000,)\n",
      "Test:  (10000, 784) (10000,)\n",
      "Validation:  (6000, 784) (6000,)\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "t = int(X_train.shape[0] * 0.9)\n",
    "X_train, X_validation = X_train[:t], X_train[t:]\n",
    "y_train, y_validation = y_train[:t], y_train[t:]\n",
    "\n",
    "print(\"Train: \", X_train.shape, y_train.shape)\n",
    "print(\"Test: \", X_test.shape, y_test.shape)\n",
    "print(\"Validation: \", X_validation.shape, y_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_input = X_train.shape[1]\n",
    "n_output = 10\n",
    "n_hidden1 = 64\n",
    "n_hidden2 = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W1_ = np.random.randn(n_input, n_hidden1)\n",
    "b1_ = np.random.randn(n_hidden1)\n",
    "\n",
    "W2_ = np.random.randn(n_hidden1, n_hidden2)\n",
    "b2_ = np.random.randn(n_hidden2)\n",
    "\n",
    "W3_ = np.random.randn(n_hidden2, n_output)\n",
    "b3_ = np.random.randn(n_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = Input(name='X'), Input(name='y')  \n",
    "W1, b1 = Input(name='W1'), Input(name='b1')  \n",
    "W2, b2 = Input(name='W2'), Input(name='b2')  \n",
    "W3, b3 = Input(name='W3'), Input(name='b3')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l1 = Linear(X, W1, b1, name='l1')\n",
    "s1 = Sigmoid(l1, name='s1')\n",
    "l2 = Linear(s1, W2, b2, name='l2')\n",
    "s2 = Sigmoid(l2, name='s2')\n",
    "l3 = Linear(s2, W3, b3, name='l3')\n",
    "cost = SoftmaxCrossEntropy(y, l3, name='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feed_dict = {\n",
    "    X: X_train,\n",
    "    y: y_train,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_,\n",
    "    W3: W3_,\n",
    "    b3: b3_\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "epochs = 200\n",
    "lr = 0.001\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = X_train.shape[0]\n",
    "steps_per_epoch = m // batch_size\n",
    "graph = topological_sort(feed_dict)\n",
    "trainables = [W1, b1, W2, b2, W3, b3]\n",
    "losses = {'train': [], 'validation': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 10.889, Validation Loss: 7.690\n",
      "Epoch: 2, Train Loss: 6.664, Validation Loss: 5.846\n",
      "Epoch: 3, Train Loss: 5.333, Validation Loss: 4.905\n",
      "Epoch: 4, Train Loss: 4.596, Validation Loss: 4.251\n",
      "Epoch: 5, Train Loss: 3.964, Validation Loss: 3.688\n",
      "Epoch: 6, Train Loss: 3.517, Validation Loss: 3.316\n",
      "Epoch: 7, Train Loss: 3.227, Validation Loss: 3.086\n",
      "Epoch: 8, Train Loss: 3.013, Validation Loss: 2.899\n",
      "Epoch: 9, Train Loss: 2.880, Validation Loss: 2.733\n",
      "Epoch: 10, Train Loss: 2.718, Validation Loss: 2.585\n",
      "Epoch: 11, Train Loss: 2.587, Validation Loss: 2.454\n",
      "Epoch: 12, Train Loss: 2.480, Validation Loss: 2.336\n",
      "Epoch: 13, Train Loss: 2.359, Validation Loss: 2.230\n",
      "Epoch: 14, Train Loss: 2.271, Validation Loss: 2.135\n",
      "Epoch: 15, Train Loss: 2.170, Validation Loss: 2.049\n",
      "Epoch: 16, Train Loss: 2.103, Validation Loss: 1.972\n",
      "Epoch: 17, Train Loss: 2.025, Validation Loss: 1.901\n",
      "Epoch: 18, Train Loss: 1.981, Validation Loss: 1.836\n",
      "Epoch: 19, Train Loss: 1.903, Validation Loss: 1.777\n",
      "Epoch: 20, Train Loss: 1.853, Validation Loss: 1.722\n",
      "Epoch: 21, Train Loss: 1.809, Validation Loss: 1.671\n",
      "Epoch: 22, Train Loss: 1.748, Validation Loss: 1.625\n",
      "Epoch: 23, Train Loss: 1.711, Validation Loss: 1.583\n",
      "Epoch: 24, Train Loss: 1.662, Validation Loss: 1.543\n",
      "Epoch: 25, Train Loss: 1.635, Validation Loss: 1.506\n",
      "Epoch: 26, Train Loss: 1.613, Validation Loss: 1.471\n",
      "Epoch: 27, Train Loss: 1.571, Validation Loss: 1.440\n",
      "Epoch: 28, Train Loss: 1.523, Validation Loss: 1.410\n",
      "Epoch: 29, Train Loss: 1.510, Validation Loss: 1.382\n",
      "Epoch: 30, Train Loss: 1.500, Validation Loss: 1.356\n",
      "Epoch: 31, Train Loss: 1.468, Validation Loss: 1.331\n",
      "Epoch: 32, Train Loss: 1.433, Validation Loss: 1.307\n",
      "Epoch: 33, Train Loss: 1.418, Validation Loss: 1.286\n",
      "Epoch: 34, Train Loss: 1.387, Validation Loss: 1.266\n",
      "Epoch: 35, Train Loss: 1.382, Validation Loss: 1.246\n",
      "Epoch: 36, Train Loss: 1.357, Validation Loss: 1.226\n",
      "Epoch: 37, Train Loss: 1.334, Validation Loss: 1.210\n",
      "Epoch: 38, Train Loss: 1.317, Validation Loss: 1.193\n",
      "Epoch: 39, Train Loss: 1.306, Validation Loss: 1.177\n",
      "Epoch: 40, Train Loss: 1.298, Validation Loss: 1.161\n",
      "Epoch: 41, Train Loss: 1.276, Validation Loss: 1.146\n",
      "Epoch: 42, Train Loss: 1.265, Validation Loss: 1.132\n",
      "Epoch: 43, Train Loss: 1.243, Validation Loss: 1.119\n",
      "Epoch: 44, Train Loss: 1.220, Validation Loss: 1.106\n",
      "Epoch: 45, Train Loss: 1.210, Validation Loss: 1.094\n",
      "Epoch: 46, Train Loss: 1.201, Validation Loss: 1.083\n",
      "Epoch: 47, Train Loss: 1.198, Validation Loss: 1.071\n",
      "Epoch: 48, Train Loss: 1.188, Validation Loss: 1.060\n",
      "Epoch: 49, Train Loss: 1.175, Validation Loss: 1.050\n",
      "Epoch: 50, Train Loss: 1.174, Validation Loss: 1.040\n",
      "Epoch: 51, Train Loss: 1.151, Validation Loss: 1.030\n",
      "Epoch: 52, Train Loss: 1.141, Validation Loss: 1.020\n",
      "Epoch: 53, Train Loss: 1.141, Validation Loss: 1.011\n",
      "Epoch: 54, Train Loss: 1.125, Validation Loss: 1.002\n",
      "Epoch: 55, Train Loss: 1.117, Validation Loss: 0.994\n",
      "Epoch: 56, Train Loss: 1.099, Validation Loss: 0.985\n",
      "Epoch: 57, Train Loss: 1.097, Validation Loss: 0.978\n",
      "Epoch: 58, Train Loss: 1.102, Validation Loss: 0.969\n",
      "Epoch: 59, Train Loss: 1.083, Validation Loss: 0.962\n",
      "Epoch: 60, Train Loss: 1.081, Validation Loss: 0.954\n",
      "Epoch: 61, Train Loss: 1.073, Validation Loss: 0.947\n",
      "Epoch: 62, Train Loss: 1.071, Validation Loss: 0.940\n",
      "Epoch: 63, Train Loss: 1.051, Validation Loss: 0.934\n",
      "Epoch: 64, Train Loss: 1.046, Validation Loss: 0.927\n",
      "Epoch: 65, Train Loss: 1.042, Validation Loss: 0.920\n",
      "Epoch: 66, Train Loss: 1.040, Validation Loss: 0.914\n",
      "Epoch: 67, Train Loss: 1.025, Validation Loss: 0.908\n",
      "Epoch: 68, Train Loss: 1.024, Validation Loss: 0.902\n",
      "Epoch: 69, Train Loss: 1.020, Validation Loss: 0.896\n",
      "Epoch: 70, Train Loss: 1.010, Validation Loss: 0.890\n",
      "Epoch: 71, Train Loss: 0.994, Validation Loss: 0.884\n",
      "Epoch: 72, Train Loss: 0.996, Validation Loss: 0.879\n",
      "Epoch: 73, Train Loss: 1.003, Validation Loss: 0.873\n",
      "Epoch: 74, Train Loss: 0.991, Validation Loss: 0.869\n",
      "Epoch: 75, Train Loss: 0.985, Validation Loss: 0.864\n",
      "Epoch: 76, Train Loss: 0.979, Validation Loss: 0.859\n",
      "Epoch: 77, Train Loss: 0.969, Validation Loss: 0.854\n",
      "Epoch: 78, Train Loss: 0.972, Validation Loss: 0.849\n",
      "Epoch: 79, Train Loss: 0.965, Validation Loss: 0.844\n",
      "Epoch: 80, Train Loss: 0.961, Validation Loss: 0.840\n",
      "Epoch: 81, Train Loss: 0.951, Validation Loss: 0.836\n",
      "Epoch: 82, Train Loss: 0.951, Validation Loss: 0.831\n",
      "Epoch: 83, Train Loss: 0.947, Validation Loss: 0.827\n",
      "Epoch: 84, Train Loss: 0.952, Validation Loss: 0.822\n",
      "Epoch: 85, Train Loss: 0.932, Validation Loss: 0.819\n",
      "Epoch: 86, Train Loss: 0.922, Validation Loss: 0.815\n",
      "Epoch: 87, Train Loss: 0.925, Validation Loss: 0.811\n",
      "Epoch: 88, Train Loss: 0.921, Validation Loss: 0.807\n",
      "Epoch: 89, Train Loss: 0.912, Validation Loss: 0.803\n",
      "Epoch: 90, Train Loss: 0.916, Validation Loss: 0.799\n",
      "Epoch: 91, Train Loss: 0.906, Validation Loss: 0.795\n",
      "Epoch: 92, Train Loss: 0.909, Validation Loss: 0.791\n",
      "Epoch: 93, Train Loss: 0.910, Validation Loss: 0.788\n",
      "Epoch: 94, Train Loss: 0.902, Validation Loss: 0.784\n",
      "Epoch: 95, Train Loss: 0.901, Validation Loss: 0.781\n",
      "Epoch: 96, Train Loss: 0.888, Validation Loss: 0.777\n",
      "Epoch: 97, Train Loss: 0.883, Validation Loss: 0.774\n",
      "Epoch: 98, Train Loss: 0.887, Validation Loss: 0.770\n",
      "Epoch: 99, Train Loss: 0.876, Validation Loss: 0.767\n",
      "Epoch: 100, Train Loss: 0.885, Validation Loss: 0.764\n",
      "Epoch: 101, Train Loss: 0.868, Validation Loss: 0.761\n",
      "Epoch: 102, Train Loss: 0.868, Validation Loss: 0.758\n",
      "Epoch: 103, Train Loss: 0.865, Validation Loss: 0.755\n",
      "Epoch: 104, Train Loss: 0.881, Validation Loss: 0.751\n",
      "Epoch: 105, Train Loss: 0.861, Validation Loss: 0.749\n",
      "Epoch: 106, Train Loss: 0.858, Validation Loss: 0.746\n",
      "Epoch: 107, Train Loss: 0.845, Validation Loss: 0.743\n",
      "Epoch: 108, Train Loss: 0.841, Validation Loss: 0.741\n",
      "Epoch: 109, Train Loss: 0.846, Validation Loss: 0.737\n",
      "Epoch: 110, Train Loss: 0.843, Validation Loss: 0.734\n",
      "Epoch: 111, Train Loss: 0.842, Validation Loss: 0.732\n",
      "Epoch: 112, Train Loss: 0.842, Validation Loss: 0.729\n",
      "Epoch: 113, Train Loss: 0.835, Validation Loss: 0.726\n",
      "Epoch: 114, Train Loss: 0.828, Validation Loss: 0.724\n",
      "Epoch: 115, Train Loss: 0.830, Validation Loss: 0.721\n",
      "Epoch: 116, Train Loss: 0.827, Validation Loss: 0.719\n",
      "Epoch: 117, Train Loss: 0.824, Validation Loss: 0.716\n",
      "Epoch: 118, Train Loss: 0.829, Validation Loss: 0.714\n",
      "Epoch: 119, Train Loss: 0.826, Validation Loss: 0.712\n",
      "Epoch: 120, Train Loss: 0.807, Validation Loss: 0.709\n",
      "Epoch: 121, Train Loss: 0.823, Validation Loss: 0.707\n",
      "Epoch: 122, Train Loss: 0.814, Validation Loss: 0.704\n",
      "Epoch: 123, Train Loss: 0.805, Validation Loss: 0.702\n",
      "Epoch: 124, Train Loss: 0.801, Validation Loss: 0.700\n",
      "Epoch: 125, Train Loss: 0.794, Validation Loss: 0.697\n",
      "Epoch: 126, Train Loss: 0.793, Validation Loss: 0.695\n",
      "Epoch: 127, Train Loss: 0.801, Validation Loss: 0.693\n",
      "Epoch: 128, Train Loss: 0.802, Validation Loss: 0.691\n",
      "Epoch: 129, Train Loss: 0.796, Validation Loss: 0.689\n",
      "Epoch: 130, Train Loss: 0.796, Validation Loss: 0.687\n",
      "Epoch: 131, Train Loss: 0.791, Validation Loss: 0.685\n",
      "Epoch: 132, Train Loss: 0.791, Validation Loss: 0.682\n",
      "Epoch: 133, Train Loss: 0.786, Validation Loss: 0.681\n",
      "Epoch: 134, Train Loss: 0.785, Validation Loss: 0.678\n",
      "Epoch: 135, Train Loss: 0.792, Validation Loss: 0.676\n",
      "Epoch: 136, Train Loss: 0.778, Validation Loss: 0.674\n",
      "Epoch: 137, Train Loss: 0.770, Validation Loss: 0.673\n",
      "Epoch: 138, Train Loss: 0.773, Validation Loss: 0.670\n",
      "Epoch: 139, Train Loss: 0.777, Validation Loss: 0.668\n",
      "Epoch: 140, Train Loss: 0.774, Validation Loss: 0.667\n",
      "Epoch: 141, Train Loss: 0.773, Validation Loss: 0.665\n",
      "Epoch: 142, Train Loss: 0.771, Validation Loss: 0.663\n",
      "Epoch: 143, Train Loss: 0.770, Validation Loss: 0.661\n",
      "Epoch: 144, Train Loss: 0.759, Validation Loss: 0.659\n",
      "Epoch: 145, Train Loss: 0.758, Validation Loss: 0.657\n",
      "Epoch: 146, Train Loss: 0.757, Validation Loss: 0.656\n",
      "Epoch: 147, Train Loss: 0.751, Validation Loss: 0.654\n",
      "Epoch: 148, Train Loss: 0.758, Validation Loss: 0.652\n",
      "Epoch: 149, Train Loss: 0.743, Validation Loss: 0.650\n",
      "Epoch: 150, Train Loss: 0.748, Validation Loss: 0.649\n",
      "Epoch: 151, Train Loss: 0.736, Validation Loss: 0.647\n",
      "Epoch: 152, Train Loss: 0.749, Validation Loss: 0.645\n",
      "Epoch: 153, Train Loss: 0.749, Validation Loss: 0.644\n",
      "Epoch: 154, Train Loss: 0.748, Validation Loss: 0.642\n",
      "Epoch: 155, Train Loss: 0.746, Validation Loss: 0.640\n",
      "Epoch: 156, Train Loss: 0.741, Validation Loss: 0.639\n",
      "Epoch: 157, Train Loss: 0.740, Validation Loss: 0.637\n",
      "Epoch: 158, Train Loss: 0.738, Validation Loss: 0.636\n",
      "Epoch: 159, Train Loss: 0.727, Validation Loss: 0.634\n",
      "Epoch: 160, Train Loss: 0.733, Validation Loss: 0.633\n",
      "Epoch: 161, Train Loss: 0.731, Validation Loss: 0.631\n",
      "Epoch: 162, Train Loss: 0.726, Validation Loss: 0.630\n",
      "Epoch: 163, Train Loss: 0.726, Validation Loss: 0.628\n",
      "Epoch: 164, Train Loss: 0.721, Validation Loss: 0.627\n",
      "Epoch: 165, Train Loss: 0.729, Validation Loss: 0.625\n",
      "Epoch: 166, Train Loss: 0.730, Validation Loss: 0.624\n",
      "Epoch: 167, Train Loss: 0.721, Validation Loss: 0.622\n",
      "Epoch: 168, Train Loss: 0.718, Validation Loss: 0.621\n",
      "Epoch: 169, Train Loss: 0.718, Validation Loss: 0.619\n",
      "Epoch: 170, Train Loss: 0.723, Validation Loss: 0.618\n",
      "Epoch: 171, Train Loss: 0.717, Validation Loss: 0.617\n",
      "Epoch: 172, Train Loss: 0.702, Validation Loss: 0.615\n",
      "Epoch: 173, Train Loss: 0.709, Validation Loss: 0.614\n",
      "Epoch: 174, Train Loss: 0.712, Validation Loss: 0.612\n",
      "Epoch: 175, Train Loss: 0.705, Validation Loss: 0.611\n",
      "Epoch: 176, Train Loss: 0.707, Validation Loss: 0.610\n",
      "Epoch: 177, Train Loss: 0.706, Validation Loss: 0.608\n",
      "Epoch: 178, Train Loss: 0.705, Validation Loss: 0.607\n",
      "Epoch: 179, Train Loss: 0.702, Validation Loss: 0.606\n",
      "Epoch: 180, Train Loss: 0.695, Validation Loss: 0.604\n",
      "Epoch: 181, Train Loss: 0.701, Validation Loss: 0.603\n",
      "Epoch: 182, Train Loss: 0.697, Validation Loss: 0.602\n",
      "Epoch: 183, Train Loss: 0.691, Validation Loss: 0.601\n",
      "Epoch: 184, Train Loss: 0.692, Validation Loss: 0.600\n",
      "Epoch: 185, Train Loss: 0.691, Validation Loss: 0.598\n",
      "Epoch: 186, Train Loss: 0.690, Validation Loss: 0.597\n",
      "Epoch: 187, Train Loss: 0.689, Validation Loss: 0.596\n",
      "Epoch: 188, Train Loss: 0.680, Validation Loss: 0.595\n",
      "Epoch: 189, Train Loss: 0.681, Validation Loss: 0.594\n",
      "Epoch: 190, Train Loss: 0.688, Validation Loss: 0.592\n",
      "Epoch: 191, Train Loss: 0.691, Validation Loss: 0.591\n",
      "Epoch: 192, Train Loss: 0.679, Validation Loss: 0.590\n",
      "Epoch: 193, Train Loss: 0.683, Validation Loss: 0.589\n",
      "Epoch: 194, Train Loss: 0.674, Validation Loss: 0.588\n",
      "Epoch: 195, Train Loss: 0.680, Validation Loss: 0.587\n",
      "Epoch: 196, Train Loss: 0.684, Validation Loss: 0.585\n",
      "Epoch: 197, Train Loss: 0.678, Validation Loss: 0.584\n",
      "Epoch: 198, Train Loss: 0.667, Validation Loss: 0.583\n",
      "Epoch: 199, Train Loss: 0.679, Validation Loss: 0.582\n",
      "Epoch: 200, Train Loss: 0.669, Validation Loss: 0.581\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for i in range(epochs):\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    for j in range(steps_per_epoch):\n",
    "        X_batch, y_batch = resample(X_train, y_train, n_samples=batch_size)\n",
    "        X.value, y.value = X_batch, y_batch\n",
    "        \n",
    "        forward_and_backward(graph)\n",
    "        sgd_update(trainables, lr)\n",
    "        train_loss += graph[-1].value\n",
    "        \n",
    "    train_loss = train_loss / steps_per_epoch\n",
    "    losses['train'].append(train_loss)\n",
    "    \n",
    "    X.value, y.value = X_validation, y_validation\n",
    "    forward_and_backward(graph, training=False)\n",
    "    val_loss = graph[-1].value\n",
    "    losses['validation'].append(val_loss)\n",
    "    \n",
    "    #if (i+1) % batch_size == 0:\n",
    "    print(\"Epoch: {}, Train Loss: {:.3f}, Validation Loss: {:.3f}\".format(i+1, train_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x11d755eb8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nT2IVigOk1gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1110ca550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x = np.arange(len(losses['train']))\n",
    "ax.plot(x, losses['train'], label='Train')\n",
    "ax.plot(x, losses['validation'], label='Validation')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.662\n",
      "Accuracy: 0.791\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "feed_dict[X] = X_test\n",
    "feed_dict[y] = y_test\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "forward_and_backward(graph, training=False)\n",
    "loss = graph[-1].value\n",
    "prediction = graph[-2].value.argmax(axis=1)\n",
    "accuracy = prediction == y_test\n",
    "accuracy = np.sum(accuracy) / accuracy.shape[0]\n",
    "\n",
    "print(\"Test loss: {:.3f}\".format(loss))\n",
    "print(\"Accuracy: {:.3f}\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
