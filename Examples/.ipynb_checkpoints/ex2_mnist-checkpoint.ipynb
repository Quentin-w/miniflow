{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle, resample\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request as req\n",
    "import gzip\n",
    "import pickle\n",
    "import sys, os\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\n",
    "from miniflow import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "dir_name = 'mnist_data'\n",
    "\n",
    "dataset = {\n",
    "    'train_img'   : 'train-images-idx3-ubyte.gz',\n",
    "    'train_label' : 'train-labels-idx1-ubyte.gz',\n",
    "    'test_img'    : 't10k-images-idx3-ubyte.gz',\n",
    "    'test_label'  : 't10k-labels-idx1-ubyte.gz',\n",
    "    }\n",
    "\n",
    "save_file = 'mnist.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(url, dname, fnames):\n",
    "    if not os.path.exists(dname):\n",
    "        os.mkdir(dname)\n",
    "    for fname in fnames:\n",
    "        fpath = os.path.join(dname, fname)\n",
    "        if not os.path.isfile(fpath):\n",
    "            print(\"Downloading\", fname, \"...\")\n",
    "            req.urlretrieve(os.path.join(url,fname) , fpath)\n",
    "    \n",
    "\n",
    "def read_image(dname, fname):\n",
    "    fpath = os.path.join(dname, fname)\n",
    "    with gzip.open(fpath, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "    data = data.reshape(-1, 784)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def read_label(dname, fname):\n",
    "    fpath = os.path.join(dname, fname)\n",
    "    with gzip.open(fpath, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "        \n",
    "    return data\n",
    "\n",
    "def create_pkl(dname, save_file, data):\n",
    "    fpath = os.path.join(dname, save_file)\n",
    "    if os.path.exists(fpath):\n",
    "        return\n",
    "    \n",
    "    with open(fpath, 'wb') as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data(data_url, dir_name, dataset.values())\n",
    "\n",
    "mnist_data = {}\n",
    "mnist_data['train_img'] = read_image(dir_name, dataset['train_img'])\n",
    "mnist_data['train_label'] = read_label(dir_name, dataset['train_label'])\n",
    "mnist_data['test_img'] = read_image(dir_name, dataset['test_img'])\n",
    "mnist_data['test_label'] = read_label(dir_name, dataset['test_label'])\n",
    "\n",
    "create_pkl(dir_name, save_file, mnist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "sfp = os.path.join(dir_name, save_file)\n",
    "with open(sfp, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "X_train, y_train = data['train_img'], data['train_label']\n",
    "X_test, y_test = data['test_img'], data['test_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalized(data):\n",
    "    data = data.astype(np.float32)\n",
    "    return data / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalized data\n",
    "X_train = normalized(X_train)\n",
    "X_test = normalized(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(data, n):\n",
    "    one_hot_encoding = np.zeros([data.shape[-1], n])\n",
    "    one_hot_encoding[np.arange(data.shape[-1]), data] = 1 \n",
    "    \n",
    "    return one_hot_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y_train = one_hot(y_train, 10)\n",
    "#y_test = one_hot(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (54000, 784) (54000,)\n",
      "Test:  (10000, 784) (10000,)\n",
      "Validation:  (6000, 784) (6000,)\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "t = int(X_train.shape[0] * 0.9)\n",
    "X_train, X_validation = X_train[:t], X_train[t:]\n",
    "y_train, y_validation = y_train[:t], y_train[t:]\n",
    "\n",
    "print(\"Train: \", X_train.shape, y_train.shape)\n",
    "print(\"Test: \", X_test.shape, y_test.shape)\n",
    "print(\"Validation: \", X_validation.shape, y_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_input = X_train.shape[1]\n",
    "n_output = 10\n",
    "n_hidden1 = 64\n",
    "n_hidden2 = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W1_ = np.random.randn(n_input, n_hidden1)\n",
    "b1_ = np.random.randn(n_hidden1)\n",
    "\n",
    "W2_ = np.random.randn(n_hidden1, n_hidden2)\n",
    "b2_ = np.random.randn(n_hidden2)\n",
    "\n",
    "W3_ = np.random.randn(n_hidden2, n_output)\n",
    "b3_ = np.random.randn(n_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = Input(name='X'), Input(name='y')  \n",
    "W1, b1 = Input(name='W1'), Input(name='b1')  \n",
    "W2, b2 = Input(name='W2'), Input(name='b2')  \n",
    "W3, b3 = Input(name='W3'), Input(name='b3')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l1 = Linear(X, W1, b1, name='l1')\n",
    "s1 = Sigmoid(l1, name='s1')\n",
    "l2 = Linear(s1, W2, b2, name='l2')\n",
    "s2 = Sigmoid(l2, name='s2')\n",
    "l3 = Linear(s2, W3, b3, name='l3')\n",
    "cost = SoftmaxCrossEntropy(y, l3, name='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feed_dict = {\n",
    "    X: X_train,\n",
    "    y: y_train,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_,\n",
    "    W3: W3_,\n",
    "    b3: b3_\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "epochs = 300\n",
    "lr = 0.001\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = X_train.shape[0]\n",
    "steps_per_epoch = m // batch_size\n",
    "graph = topological_sort(feed_dict)\n",
    "trainables = [W1, b1, W2, b2, W3, b3]\n",
    "losses = {'train': [], 'validation': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 6.488, Validation Loss: 5.239\n",
      "Epoch: 2, Train Loss: 4.594, Validation Loss: 4.328\n",
      "Epoch: 3, Train Loss: 4.031, Validation Loss: 3.951\n",
      "Epoch: 4, Train Loss: 3.738, Validation Loss: 3.670\n",
      "Epoch: 5, Train Loss: 3.460, Validation Loss: 3.422\n",
      "Epoch: 6, Train Loss: 3.238, Validation Loss: 3.202\n",
      "Epoch: 7, Train Loss: 3.054, Validation Loss: 3.009\n",
      "Epoch: 8, Train Loss: 2.901, Validation Loss: 2.839\n",
      "Epoch: 9, Train Loss: 2.752, Validation Loss: 2.688\n",
      "Epoch: 10, Train Loss: 2.617, Validation Loss: 2.554\n",
      "Epoch: 11, Train Loss: 2.500, Validation Loss: 2.437\n",
      "Epoch: 12, Train Loss: 2.404, Validation Loss: 2.330\n",
      "Epoch: 13, Train Loss: 2.307, Validation Loss: 2.235\n",
      "Epoch: 14, Train Loss: 2.221, Validation Loss: 2.148\n",
      "Epoch: 15, Train Loss: 2.152, Validation Loss: 2.070\n",
      "Epoch: 16, Train Loss: 2.071, Validation Loss: 1.999\n",
      "Epoch: 17, Train Loss: 2.010, Validation Loss: 1.934\n",
      "Epoch: 18, Train Loss: 1.957, Validation Loss: 1.875\n",
      "Epoch: 19, Train Loss: 1.885, Validation Loss: 1.821\n",
      "Epoch: 20, Train Loss: 1.830, Validation Loss: 1.772\n",
      "Epoch: 21, Train Loss: 1.799, Validation Loss: 1.726\n",
      "Epoch: 22, Train Loss: 1.748, Validation Loss: 1.683\n",
      "Epoch: 23, Train Loss: 1.708, Validation Loss: 1.644\n",
      "Epoch: 24, Train Loss: 1.696, Validation Loss: 1.607\n",
      "Epoch: 25, Train Loss: 1.643, Validation Loss: 1.573\n",
      "Epoch: 26, Train Loss: 1.608, Validation Loss: 1.541\n",
      "Epoch: 27, Train Loss: 1.589, Validation Loss: 1.511\n",
      "Epoch: 28, Train Loss: 1.562, Validation Loss: 1.483\n",
      "Epoch: 29, Train Loss: 1.528, Validation Loss: 1.457\n",
      "Epoch: 30, Train Loss: 1.508, Validation Loss: 1.432\n",
      "Epoch: 31, Train Loss: 1.480, Validation Loss: 1.408\n",
      "Epoch: 32, Train Loss: 1.457, Validation Loss: 1.386\n",
      "Epoch: 33, Train Loss: 1.437, Validation Loss: 1.365\n",
      "Epoch: 34, Train Loss: 1.413, Validation Loss: 1.344\n",
      "Epoch: 35, Train Loss: 1.416, Validation Loss: 1.325\n",
      "Epoch: 36, Train Loss: 1.373, Validation Loss: 1.307\n",
      "Epoch: 37, Train Loss: 1.353, Validation Loss: 1.290\n",
      "Epoch: 38, Train Loss: 1.337, Validation Loss: 1.273\n",
      "Epoch: 39, Train Loss: 1.330, Validation Loss: 1.257\n",
      "Epoch: 40, Train Loss: 1.319, Validation Loss: 1.242\n",
      "Epoch: 41, Train Loss: 1.298, Validation Loss: 1.228\n",
      "Epoch: 42, Train Loss: 1.298, Validation Loss: 1.213\n",
      "Epoch: 43, Train Loss: 1.279, Validation Loss: 1.200\n",
      "Epoch: 44, Train Loss: 1.261, Validation Loss: 1.187\n",
      "Epoch: 45, Train Loss: 1.248, Validation Loss: 1.175\n",
      "Epoch: 46, Train Loss: 1.239, Validation Loss: 1.162\n",
      "Epoch: 47, Train Loss: 1.239, Validation Loss: 1.151\n",
      "Epoch: 48, Train Loss: 1.215, Validation Loss: 1.140\n",
      "Epoch: 49, Train Loss: 1.195, Validation Loss: 1.129\n",
      "Epoch: 50, Train Loss: 1.199, Validation Loss: 1.119\n",
      "Epoch: 51, Train Loss: 1.181, Validation Loss: 1.109\n",
      "Epoch: 52, Train Loss: 1.168, Validation Loss: 1.099\n",
      "Epoch: 53, Train Loss: 1.155, Validation Loss: 1.090\n",
      "Epoch: 54, Train Loss: 1.150, Validation Loss: 1.081\n",
      "Epoch: 55, Train Loss: 1.151, Validation Loss: 1.072\n",
      "Epoch: 56, Train Loss: 1.139, Validation Loss: 1.064\n",
      "Epoch: 57, Train Loss: 1.118, Validation Loss: 1.055\n",
      "Epoch: 58, Train Loss: 1.122, Validation Loss: 1.047\n",
      "Epoch: 59, Train Loss: 1.103, Validation Loss: 1.039\n",
      "Epoch: 60, Train Loss: 1.114, Validation Loss: 1.031\n",
      "Epoch: 61, Train Loss: 1.093, Validation Loss: 1.024\n",
      "Epoch: 62, Train Loss: 1.084, Validation Loss: 1.017\n",
      "Epoch: 63, Train Loss: 1.083, Validation Loss: 1.010\n",
      "Epoch: 64, Train Loss: 1.080, Validation Loss: 1.003\n",
      "Epoch: 65, Train Loss: 1.074, Validation Loss: 0.996\n",
      "Epoch: 66, Train Loss: 1.066, Validation Loss: 0.990\n",
      "Epoch: 67, Train Loss: 1.055, Validation Loss: 0.983\n",
      "Epoch: 68, Train Loss: 1.058, Validation Loss: 0.977\n",
      "Epoch: 69, Train Loss: 1.031, Validation Loss: 0.971\n",
      "Epoch: 70, Train Loss: 1.047, Validation Loss: 0.965\n",
      "Epoch: 71, Train Loss: 1.026, Validation Loss: 0.959\n",
      "Epoch: 72, Train Loss: 1.022, Validation Loss: 0.953\n",
      "Epoch: 73, Train Loss: 1.021, Validation Loss: 0.948\n",
      "Epoch: 74, Train Loss: 1.025, Validation Loss: 0.942\n",
      "Epoch: 75, Train Loss: 1.021, Validation Loss: 0.937\n",
      "Epoch: 76, Train Loss: 1.005, Validation Loss: 0.931\n",
      "Epoch: 77, Train Loss: 0.998, Validation Loss: 0.926\n",
      "Epoch: 78, Train Loss: 0.992, Validation Loss: 0.921\n",
      "Epoch: 79, Train Loss: 0.999, Validation Loss: 0.916\n",
      "Epoch: 80, Train Loss: 0.985, Validation Loss: 0.912\n",
      "Epoch: 81, Train Loss: 0.986, Validation Loss: 0.907\n",
      "Epoch: 82, Train Loss: 0.980, Validation Loss: 0.902\n",
      "Epoch: 83, Train Loss: 0.958, Validation Loss: 0.897\n",
      "Epoch: 84, Train Loss: 0.965, Validation Loss: 0.893\n",
      "Epoch: 85, Train Loss: 0.956, Validation Loss: 0.889\n",
      "Epoch: 86, Train Loss: 0.954, Validation Loss: 0.884\n",
      "Epoch: 87, Train Loss: 0.958, Validation Loss: 0.880\n",
      "Epoch: 88, Train Loss: 0.946, Validation Loss: 0.876\n",
      "Epoch: 89, Train Loss: 0.942, Validation Loss: 0.872\n",
      "Epoch: 90, Train Loss: 0.937, Validation Loss: 0.868\n",
      "Epoch: 91, Train Loss: 0.928, Validation Loss: 0.864\n",
      "Epoch: 92, Train Loss: 0.935, Validation Loss: 0.860\n",
      "Epoch: 93, Train Loss: 0.933, Validation Loss: 0.856\n",
      "Epoch: 94, Train Loss: 0.933, Validation Loss: 0.852\n",
      "Epoch: 95, Train Loss: 0.914, Validation Loss: 0.849\n",
      "Epoch: 96, Train Loss: 0.913, Validation Loss: 0.845\n",
      "Epoch: 97, Train Loss: 0.908, Validation Loss: 0.841\n",
      "Epoch: 98, Train Loss: 0.902, Validation Loss: 0.838\n",
      "Epoch: 99, Train Loss: 0.907, Validation Loss: 0.835\n",
      "Epoch: 100, Train Loss: 0.912, Validation Loss: 0.831\n",
      "Epoch: 101, Train Loss: 0.900, Validation Loss: 0.828\n",
      "Epoch: 102, Train Loss: 0.894, Validation Loss: 0.825\n",
      "Epoch: 103, Train Loss: 0.891, Validation Loss: 0.821\n",
      "Epoch: 104, Train Loss: 0.892, Validation Loss: 0.818\n",
      "Epoch: 105, Train Loss: 0.878, Validation Loss: 0.815\n",
      "Epoch: 106, Train Loss: 0.888, Validation Loss: 0.812\n",
      "Epoch: 107, Train Loss: 0.879, Validation Loss: 0.809\n",
      "Epoch: 108, Train Loss: 0.869, Validation Loss: 0.806\n",
      "Epoch: 109, Train Loss: 0.868, Validation Loss: 0.803\n",
      "Epoch: 110, Train Loss: 0.865, Validation Loss: 0.800\n",
      "Epoch: 111, Train Loss: 0.866, Validation Loss: 0.797\n",
      "Epoch: 112, Train Loss: 0.863, Validation Loss: 0.794\n",
      "Epoch: 113, Train Loss: 0.855, Validation Loss: 0.791\n",
      "Epoch: 114, Train Loss: 0.859, Validation Loss: 0.788\n",
      "Epoch: 115, Train Loss: 0.854, Validation Loss: 0.786\n",
      "Epoch: 116, Train Loss: 0.855, Validation Loss: 0.783\n",
      "Epoch: 117, Train Loss: 0.836, Validation Loss: 0.780\n",
      "Epoch: 118, Train Loss: 0.849, Validation Loss: 0.777\n",
      "Epoch: 119, Train Loss: 0.840, Validation Loss: 0.775\n",
      "Epoch: 120, Train Loss: 0.848, Validation Loss: 0.772\n",
      "Epoch: 121, Train Loss: 0.836, Validation Loss: 0.769\n",
      "Epoch: 122, Train Loss: 0.833, Validation Loss: 0.767\n",
      "Epoch: 123, Train Loss: 0.831, Validation Loss: 0.765\n",
      "Epoch: 124, Train Loss: 0.827, Validation Loss: 0.762\n",
      "Epoch: 125, Train Loss: 0.822, Validation Loss: 0.759\n",
      "Epoch: 126, Train Loss: 0.814, Validation Loss: 0.757\n",
      "Epoch: 127, Train Loss: 0.825, Validation Loss: 0.755\n",
      "Epoch: 128, Train Loss: 0.815, Validation Loss: 0.752\n",
      "Epoch: 129, Train Loss: 0.814, Validation Loss: 0.750\n",
      "Epoch: 130, Train Loss: 0.806, Validation Loss: 0.748\n",
      "Epoch: 131, Train Loss: 0.810, Validation Loss: 0.745\n",
      "Epoch: 132, Train Loss: 0.811, Validation Loss: 0.743\n",
      "Epoch: 133, Train Loss: 0.810, Validation Loss: 0.741\n",
      "Epoch: 134, Train Loss: 0.806, Validation Loss: 0.738\n",
      "Epoch: 135, Train Loss: 0.813, Validation Loss: 0.736\n",
      "Epoch: 136, Train Loss: 0.794, Validation Loss: 0.734\n",
      "Epoch: 137, Train Loss: 0.790, Validation Loss: 0.732\n",
      "Epoch: 138, Train Loss: 0.801, Validation Loss: 0.730\n",
      "Epoch: 139, Train Loss: 0.797, Validation Loss: 0.728\n",
      "Epoch: 140, Train Loss: 0.791, Validation Loss: 0.725\n",
      "Epoch: 141, Train Loss: 0.789, Validation Loss: 0.723\n",
      "Epoch: 142, Train Loss: 0.792, Validation Loss: 0.722\n",
      "Epoch: 143, Train Loss: 0.783, Validation Loss: 0.719\n",
      "Epoch: 144, Train Loss: 0.790, Validation Loss: 0.717\n",
      "Epoch: 145, Train Loss: 0.793, Validation Loss: 0.715\n",
      "Epoch: 146, Train Loss: 0.778, Validation Loss: 0.714\n",
      "Epoch: 147, Train Loss: 0.770, Validation Loss: 0.712\n",
      "Epoch: 148, Train Loss: 0.772, Validation Loss: 0.709\n",
      "Epoch: 149, Train Loss: 0.778, Validation Loss: 0.707\n",
      "Epoch: 150, Train Loss: 0.777, Validation Loss: 0.706\n",
      "Epoch: 151, Train Loss: 0.770, Validation Loss: 0.704\n",
      "Epoch: 152, Train Loss: 0.762, Validation Loss: 0.702\n",
      "Epoch: 153, Train Loss: 0.773, Validation Loss: 0.700\n",
      "Epoch: 154, Train Loss: 0.761, Validation Loss: 0.698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 155, Train Loss: 0.760, Validation Loss: 0.697\n",
      "Epoch: 156, Train Loss: 0.759, Validation Loss: 0.695\n",
      "Epoch: 157, Train Loss: 0.759, Validation Loss: 0.693\n",
      "Epoch: 158, Train Loss: 0.753, Validation Loss: 0.691\n",
      "Epoch: 159, Train Loss: 0.760, Validation Loss: 0.690\n",
      "Epoch: 160, Train Loss: 0.752, Validation Loss: 0.688\n",
      "Epoch: 161, Train Loss: 0.759, Validation Loss: 0.686\n",
      "Epoch: 162, Train Loss: 0.744, Validation Loss: 0.684\n",
      "Epoch: 163, Train Loss: 0.748, Validation Loss: 0.683\n",
      "Epoch: 164, Train Loss: 0.749, Validation Loss: 0.681\n",
      "Epoch: 165, Train Loss: 0.744, Validation Loss: 0.680\n",
      "Epoch: 166, Train Loss: 0.744, Validation Loss: 0.678\n",
      "Epoch: 167, Train Loss: 0.750, Validation Loss: 0.676\n",
      "Epoch: 168, Train Loss: 0.740, Validation Loss: 0.675\n",
      "Epoch: 169, Train Loss: 0.737, Validation Loss: 0.673\n",
      "Epoch: 170, Train Loss: 0.732, Validation Loss: 0.671\n",
      "Epoch: 171, Train Loss: 0.726, Validation Loss: 0.670\n",
      "Epoch: 172, Train Loss: 0.728, Validation Loss: 0.668\n",
      "Epoch: 173, Train Loss: 0.720, Validation Loss: 0.667\n",
      "Epoch: 174, Train Loss: 0.736, Validation Loss: 0.665\n",
      "Epoch: 175, Train Loss: 0.728, Validation Loss: 0.664\n",
      "Epoch: 176, Train Loss: 0.733, Validation Loss: 0.662\n",
      "Epoch: 177, Train Loss: 0.730, Validation Loss: 0.661\n",
      "Epoch: 178, Train Loss: 0.720, Validation Loss: 0.659\n",
      "Epoch: 179, Train Loss: 0.714, Validation Loss: 0.658\n",
      "Epoch: 180, Train Loss: 0.718, Validation Loss: 0.656\n",
      "Epoch: 181, Train Loss: 0.729, Validation Loss: 0.655\n",
      "Epoch: 182, Train Loss: 0.717, Validation Loss: 0.653\n",
      "Epoch: 183, Train Loss: 0.710, Validation Loss: 0.652\n",
      "Epoch: 184, Train Loss: 0.707, Validation Loss: 0.651\n",
      "Epoch: 185, Train Loss: 0.710, Validation Loss: 0.649\n",
      "Epoch: 186, Train Loss: 0.707, Validation Loss: 0.648\n",
      "Epoch: 187, Train Loss: 0.711, Validation Loss: 0.646\n",
      "Epoch: 188, Train Loss: 0.705, Validation Loss: 0.645\n",
      "Epoch: 189, Train Loss: 0.707, Validation Loss: 0.644\n",
      "Epoch: 190, Train Loss: 0.707, Validation Loss: 0.643\n",
      "Epoch: 191, Train Loss: 0.701, Validation Loss: 0.641\n",
      "Epoch: 192, Train Loss: 0.703, Validation Loss: 0.640\n",
      "Epoch: 193, Train Loss: 0.694, Validation Loss: 0.639\n",
      "Epoch: 194, Train Loss: 0.703, Validation Loss: 0.637\n",
      "Epoch: 195, Train Loss: 0.699, Validation Loss: 0.636\n",
      "Epoch: 196, Train Loss: 0.698, Validation Loss: 0.635\n",
      "Epoch: 197, Train Loss: 0.706, Validation Loss: 0.633\n",
      "Epoch: 198, Train Loss: 0.684, Validation Loss: 0.632\n",
      "Epoch: 199, Train Loss: 0.689, Validation Loss: 0.631\n",
      "Epoch: 200, Train Loss: 0.685, Validation Loss: 0.629\n",
      "Epoch: 201, Train Loss: 0.684, Validation Loss: 0.628\n",
      "Epoch: 202, Train Loss: 0.691, Validation Loss: 0.627\n",
      "Epoch: 203, Train Loss: 0.687, Validation Loss: 0.626\n",
      "Epoch: 204, Train Loss: 0.682, Validation Loss: 0.625\n",
      "Epoch: 205, Train Loss: 0.688, Validation Loss: 0.624\n",
      "Epoch: 206, Train Loss: 0.674, Validation Loss: 0.623\n",
      "Epoch: 207, Train Loss: 0.676, Validation Loss: 0.621\n",
      "Epoch: 208, Train Loss: 0.682, Validation Loss: 0.620\n",
      "Epoch: 209, Train Loss: 0.677, Validation Loss: 0.619\n",
      "Epoch: 210, Train Loss: 0.680, Validation Loss: 0.618\n",
      "Epoch: 211, Train Loss: 0.675, Validation Loss: 0.617\n",
      "Epoch: 212, Train Loss: 0.671, Validation Loss: 0.615\n",
      "Epoch: 213, Train Loss: 0.669, Validation Loss: 0.614\n",
      "Epoch: 214, Train Loss: 0.670, Validation Loss: 0.613\n",
      "Epoch: 215, Train Loss: 0.684, Validation Loss: 0.612\n",
      "Epoch: 216, Train Loss: 0.667, Validation Loss: 0.611\n",
      "Epoch: 217, Train Loss: 0.673, Validation Loss: 0.610\n",
      "Epoch: 218, Train Loss: 0.668, Validation Loss: 0.609\n",
      "Epoch: 219, Train Loss: 0.669, Validation Loss: 0.608\n",
      "Epoch: 220, Train Loss: 0.666, Validation Loss: 0.607\n",
      "Epoch: 221, Train Loss: 0.669, Validation Loss: 0.605\n",
      "Epoch: 222, Train Loss: 0.659, Validation Loss: 0.604\n",
      "Epoch: 223, Train Loss: 0.665, Validation Loss: 0.603\n",
      "Epoch: 224, Train Loss: 0.662, Validation Loss: 0.602\n",
      "Epoch: 225, Train Loss: 0.653, Validation Loss: 0.601\n",
      "Epoch: 226, Train Loss: 0.657, Validation Loss: 0.600\n",
      "Epoch: 227, Train Loss: 0.655, Validation Loss: 0.599\n",
      "Epoch: 228, Train Loss: 0.660, Validation Loss: 0.598\n",
      "Epoch: 229, Train Loss: 0.658, Validation Loss: 0.597\n",
      "Epoch: 230, Train Loss: 0.658, Validation Loss: 0.596\n",
      "Epoch: 231, Train Loss: 0.654, Validation Loss: 0.595\n",
      "Epoch: 232, Train Loss: 0.647, Validation Loss: 0.594\n",
      "Epoch: 233, Train Loss: 0.649, Validation Loss: 0.593\n",
      "Epoch: 234, Train Loss: 0.648, Validation Loss: 0.592\n",
      "Epoch: 235, Train Loss: 0.653, Validation Loss: 0.591\n",
      "Epoch: 236, Train Loss: 0.644, Validation Loss: 0.590\n",
      "Epoch: 237, Train Loss: 0.648, Validation Loss: 0.589\n",
      "Epoch: 238, Train Loss: 0.649, Validation Loss: 0.588\n",
      "Epoch: 239, Train Loss: 0.637, Validation Loss: 0.587\n",
      "Epoch: 240, Train Loss: 0.642, Validation Loss: 0.586\n",
      "Epoch: 241, Train Loss: 0.649, Validation Loss: 0.585\n",
      "Epoch: 242, Train Loss: 0.635, Validation Loss: 0.584\n",
      "Epoch: 243, Train Loss: 0.638, Validation Loss: 0.583\n",
      "Epoch: 244, Train Loss: 0.643, Validation Loss: 0.582\n",
      "Epoch: 245, Train Loss: 0.641, Validation Loss: 0.582\n",
      "Epoch: 246, Train Loss: 0.637, Validation Loss: 0.581\n",
      "Epoch: 247, Train Loss: 0.639, Validation Loss: 0.580\n",
      "Epoch: 248, Train Loss: 0.635, Validation Loss: 0.579\n",
      "Epoch: 249, Train Loss: 0.641, Validation Loss: 0.578\n",
      "Epoch: 250, Train Loss: 0.633, Validation Loss: 0.577\n",
      "Epoch: 251, Train Loss: 0.631, Validation Loss: 0.576\n",
      "Epoch: 252, Train Loss: 0.636, Validation Loss: 0.575\n",
      "Epoch: 253, Train Loss: 0.629, Validation Loss: 0.574\n",
      "Epoch: 254, Train Loss: 0.628, Validation Loss: 0.573\n",
      "Epoch: 255, Train Loss: 0.629, Validation Loss: 0.573\n",
      "Epoch: 256, Train Loss: 0.623, Validation Loss: 0.572\n",
      "Epoch: 257, Train Loss: 0.628, Validation Loss: 0.571\n",
      "Epoch: 258, Train Loss: 0.629, Validation Loss: 0.570\n",
      "Epoch: 259, Train Loss: 0.627, Validation Loss: 0.569\n",
      "Epoch: 260, Train Loss: 0.617, Validation Loss: 0.568\n",
      "Epoch: 261, Train Loss: 0.624, Validation Loss: 0.567\n",
      "Epoch: 262, Train Loss: 0.623, Validation Loss: 0.567\n",
      "Epoch: 263, Train Loss: 0.622, Validation Loss: 0.566\n",
      "Epoch: 264, Train Loss: 0.611, Validation Loss: 0.565\n",
      "Epoch: 265, Train Loss: 0.626, Validation Loss: 0.564\n",
      "Epoch: 266, Train Loss: 0.621, Validation Loss: 0.563\n",
      "Epoch: 267, Train Loss: 0.628, Validation Loss: 0.562\n",
      "Epoch: 268, Train Loss: 0.619, Validation Loss: 0.562\n",
      "Epoch: 269, Train Loss: 0.620, Validation Loss: 0.561\n",
      "Epoch: 270, Train Loss: 0.614, Validation Loss: 0.560\n",
      "Epoch: 271, Train Loss: 0.613, Validation Loss: 0.559\n",
      "Epoch: 272, Train Loss: 0.626, Validation Loss: 0.559\n",
      "Epoch: 273, Train Loss: 0.624, Validation Loss: 0.558\n",
      "Epoch: 274, Train Loss: 0.602, Validation Loss: 0.557\n",
      "Epoch: 275, Train Loss: 0.610, Validation Loss: 0.556\n",
      "Epoch: 276, Train Loss: 0.600, Validation Loss: 0.555\n",
      "Epoch: 277, Train Loss: 0.606, Validation Loss: 0.554\n",
      "Epoch: 278, Train Loss: 0.606, Validation Loss: 0.554\n",
      "Epoch: 279, Train Loss: 0.610, Validation Loss: 0.553\n",
      "Epoch: 280, Train Loss: 0.610, Validation Loss: 0.552\n",
      "Epoch: 281, Train Loss: 0.617, Validation Loss: 0.551\n",
      "Epoch: 282, Train Loss: 0.599, Validation Loss: 0.551\n",
      "Epoch: 283, Train Loss: 0.601, Validation Loss: 0.550\n",
      "Epoch: 284, Train Loss: 0.610, Validation Loss: 0.549\n",
      "Epoch: 285, Train Loss: 0.608, Validation Loss: 0.548\n",
      "Epoch: 286, Train Loss: 0.608, Validation Loss: 0.548\n",
      "Epoch: 287, Train Loss: 0.606, Validation Loss: 0.547\n",
      "Epoch: 288, Train Loss: 0.598, Validation Loss: 0.546\n",
      "Epoch: 289, Train Loss: 0.605, Validation Loss: 0.546\n",
      "Epoch: 290, Train Loss: 0.602, Validation Loss: 0.545\n",
      "Epoch: 291, Train Loss: 0.602, Validation Loss: 0.544\n",
      "Epoch: 292, Train Loss: 0.601, Validation Loss: 0.543\n",
      "Epoch: 293, Train Loss: 0.602, Validation Loss: 0.543\n",
      "Epoch: 294, Train Loss: 0.599, Validation Loss: 0.542\n",
      "Epoch: 295, Train Loss: 0.596, Validation Loss: 0.541\n",
      "Epoch: 296, Train Loss: 0.599, Validation Loss: 0.540\n",
      "Epoch: 297, Train Loss: 0.597, Validation Loss: 0.540\n",
      "Epoch: 298, Train Loss: 0.596, Validation Loss: 0.539\n",
      "Epoch: 299, Train Loss: 0.591, Validation Loss: 0.538\n",
      "Epoch: 300, Train Loss: 0.588, Validation Loss: 0.538\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for i in range(epochs):\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    for j in range(steps_per_epoch):\n",
    "        X_batch, y_batch = resample(X_train, y_train, n_samples=batch_size)\n",
    "        X.value, y.value = X_batch, y_batch\n",
    "        \n",
    "        forward_and_backward(graph)\n",
    "        sgd_update(trainables, lr)\n",
    "        train_loss += graph[-1].value\n",
    "        \n",
    "    train_loss = train_loss / steps_per_epoch\n",
    "    losses['train'].append(train_loss)\n",
    "    \n",
    "    X.value, y.value = X_validation, y_validation\n",
    "    forward_and_backward(graph, training=False)\n",
    "    val_loss = graph[-1].value\n",
    "    losses['validation'].append(val_loss)\n",
    "    \n",
    "    #if (i+1) % batch_size == 0:\n",
    "    print(\"Epoch: {}, Train Loss: {:.3f}, Validation Loss: {:.3f}\".format(i+1, train_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x11c01ae48>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niOQYBbeISI75/7IIz0awd1U5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e1b23c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x = np.arange(len(losses['train']))\n",
    "ax.plot(x, losses['train'], label='Train')\n",
    "ax.plot(x, losses['validation'], label='Validation')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.599\n",
      "Accuracy: 0.811\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "feed_dict[X] = X_test\n",
    "feed_dict[y] = y_test\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "forward_and_backward(graph, training=False)\n",
    "loss = graph[-1].value\n",
    "prediction = graph[-2].value.argmax(axis=1)\n",
    "accuracy = prediction == y_test\n",
    "accuracy = np.sum(accuracy) / accuracy.shape[0]\n",
    "\n",
    "print(\"Test loss: {:.3f}\".format(loss))\n",
    "print(\"Accuracy: {:.3f}\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
