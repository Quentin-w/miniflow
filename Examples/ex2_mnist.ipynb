{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle, resample\n",
    "from miniflow import *\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_file = 'mnist_data/train-images-idx3-ubyte.gz'\n",
    "train_label_file = 'mnist_data/train-labels-idx1-ubyte.gz'\n",
    "test_img_file = 'mnist_data/t10k-images-idx3-ubyte.gz'\n",
    "test_label_file = 'mnist_data/t10k-labels-idx1-ubyte.gz'\n",
    "\n",
    "save_file = 'mnist_data/mnist.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(fname):\n",
    "    with gzip.open(fname, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "    data = data.reshape(-1, 784)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def read_label(fname):\n",
    "    with gzip.open(fname, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "        \n",
    "    return data\n",
    "\n",
    "def create_pkl(save_file, data):\n",
    "    if os.path.exists(save_file):\n",
    "        return\n",
    "    \n",
    "    with open(save_file, 'wb') as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = {}\n",
    "mnist_data['train_img'] = read_image(train_img_file)\n",
    "mnist_data['train_label'] = read_label(train_label_file)\n",
    "mnist_data['test_img'] = read_image(test_img_file)\n",
    "mnist_data['test_label'] = read_label(test_label_file)\n",
    "\n",
    "create_pkl(save_file, mnist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open(save_file, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "X_train, y_train = data['train_img'], data['train_label']\n",
    "X_test, y_test = data['test_img'], data['test_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized(data):\n",
    "    data = data.astype(np.float32)\n",
    "    return data / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized data\n",
    "X_train = normalized(X_train)\n",
    "X_test = normalized(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(data, n):\n",
    "    one_hot_encoding = np.zeros([data.shape[-1], n])\n",
    "    one_hot_encoding[np.arange(data.shape[-1]), data] = 1 \n",
    "    \n",
    "    return one_hot_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train = one_hot(y_train, 10)\n",
    "#y_test = one_hot(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train: ', (54000, 784), (54000,))\n",
      "('Test: ', (10000, 784), (10000,))\n",
      "('Validation: ', (6000, 784), (6000,))\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "t = int(X_train.shape[0] * 0.9)\n",
    "X_train, X_validation = X_train[:t], X_train[t:]\n",
    "y_train, y_validation = y_train[:t], y_train[t:]\n",
    "\n",
    "print(\"Train: \", X_train.shape, y_train.shape)\n",
    "print(\"Test: \", X_test.shape, y_test.shape)\n",
    "print(\"Validation: \", X_validation.shape, y_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = X_train.shape[1]\n",
    "n_output = 10\n",
    "n_hidden1 = 64\n",
    "n_hidden2 = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1_ = np.random.randn(n_input, n_hidden1)\n",
    "b1_ = np.random.randn(n_hidden1)\n",
    "\n",
    "W2_ = np.random.randn(n_hidden1, n_hidden2)\n",
    "b2_ = np.random.randn(n_hidden2)\n",
    "\n",
    "W3_ = np.random.randn(n_hidden2, n_output)\n",
    "b3_ = np.random.randn(n_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = Input(name='X'), Input(name='y')  \n",
    "W1, b1 = Input(name='W1'), Input(name='b1')  \n",
    "W2, b2 = Input(name='W2'), Input(name='b2')  \n",
    "W3, b3 = Input(name='W3'), Input(name='b3')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = Linear(X, W1, b1, name='l1')\n",
    "s1 = Sigmoid(l1, name='s1')\n",
    "l2 = Linear(s1, W2, b2, name='l2')\n",
    "s2 = Sigmoid(l2, name='s2')\n",
    "l3 = Linear(s2, W3, b3, name='l3')\n",
    "cost = SoftmaxCrossEntropy(y, l3, name='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = {\n",
    "    X: X_train,\n",
    "    y: y_train,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_,\n",
    "    W3: W3_,\n",
    "    b3: b3_\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "epochs = 5000\n",
    "lr = 0.001\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X_train.shape[0]\n",
    "steps_per_epoch = m // batch_size\n",
    "graph = topological_sort(feed_dict)\n",
    "trainables = [W1, b1, W2, b2, W3, b3]\n",
    "losses = {'train': [], 'validation': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 6.143, Validation Loss: 4.813\n",
      "Epoch: 2, Train Loss: 4.369, Validation Loss: 4.040\n",
      "Epoch: 3, Train Loss: 3.897, Validation Loss: 3.695\n",
      "Epoch: 4, Train Loss: 3.598, Validation Loss: 3.411\n",
      "Epoch: 5, Train Loss: 3.345, Validation Loss: 3.167\n",
      "Epoch: 6, Train Loss: 3.125, Validation Loss: 2.954\n",
      "Epoch: 7, Train Loss: 2.920, Validation Loss: 2.769\n",
      "Epoch: 8, Train Loss: 2.762, Validation Loss: 2.609\n",
      "Epoch: 9, Train Loss: 2.607, Validation Loss: 2.468\n",
      "Epoch: 10, Train Loss: 2.496, Validation Loss: 2.345\n",
      "Epoch: 11, Train Loss: 2.385, Validation Loss: 2.237\n",
      "Epoch: 12, Train Loss: 2.275, Validation Loss: 2.140\n",
      "Epoch: 13, Train Loss: 2.198, Validation Loss: 2.054\n",
      "Epoch: 14, Train Loss: 2.120, Validation Loss: 1.978\n",
      "Epoch: 15, Train Loss: 2.062, Validation Loss: 1.909\n",
      "Epoch: 16, Train Loss: 1.987, Validation Loss: 1.846\n",
      "Epoch: 17, Train Loss: 1.928, Validation Loss: 1.789\n",
      "Epoch: 18, Train Loss: 1.876, Validation Loss: 1.737\n",
      "Epoch: 19, Train Loss: 1.812, Validation Loss: 1.689\n",
      "Epoch: 20, Train Loss: 1.775, Validation Loss: 1.644\n",
      "Epoch: 21, Train Loss: 1.731, Validation Loss: 1.604\n",
      "Epoch: 22, Train Loss: 1.681, Validation Loss: 1.566\n",
      "Epoch: 23, Train Loss: 1.658, Validation Loss: 1.531\n",
      "Epoch: 24, Train Loss: 1.618, Validation Loss: 1.498\n",
      "Epoch: 25, Train Loss: 1.586, Validation Loss: 1.467\n",
      "Epoch: 26, Train Loss: 1.574, Validation Loss: 1.438\n",
      "Epoch: 27, Train Loss: 1.531, Validation Loss: 1.411\n",
      "Epoch: 28, Train Loss: 1.513, Validation Loss: 1.386\n",
      "Epoch: 29, Train Loss: 1.499, Validation Loss: 1.361\n",
      "Epoch: 30, Train Loss: 1.443, Validation Loss: 1.338\n",
      "Epoch: 31, Train Loss: 1.443, Validation Loss: 1.316\n",
      "Epoch: 32, Train Loss: 1.420, Validation Loss: 1.296\n",
      "Epoch: 33, Train Loss: 1.410, Validation Loss: 1.276\n",
      "Epoch: 34, Train Loss: 1.383, Validation Loss: 1.258\n",
      "Epoch: 35, Train Loss: 1.357, Validation Loss: 1.240\n",
      "Epoch: 36, Train Loss: 1.346, Validation Loss: 1.223\n",
      "Epoch: 37, Train Loss: 1.338, Validation Loss: 1.207\n",
      "Epoch: 38, Train Loss: 1.308, Validation Loss: 1.192\n",
      "Epoch: 39, Train Loss: 1.301, Validation Loss: 1.177\n",
      "Epoch: 40, Train Loss: 1.272, Validation Loss: 1.162\n",
      "Epoch: 41, Train Loss: 1.269, Validation Loss: 1.149\n",
      "Epoch: 42, Train Loss: 1.254, Validation Loss: 1.136\n",
      "Epoch: 43, Train Loss: 1.238, Validation Loss: 1.123\n",
      "Epoch: 44, Train Loss: 1.241, Validation Loss: 1.112\n",
      "Epoch: 45, Train Loss: 1.214, Validation Loss: 1.100\n",
      "Epoch: 46, Train Loss: 1.208, Validation Loss: 1.088\n",
      "Epoch: 47, Train Loss: 1.194, Validation Loss: 1.078\n",
      "Epoch: 48, Train Loss: 1.186, Validation Loss: 1.067\n",
      "Epoch: 49, Train Loss: 1.163, Validation Loss: 1.057\n",
      "Epoch: 50, Train Loss: 1.174, Validation Loss: 1.047\n",
      "Epoch: 51, Train Loss: 1.149, Validation Loss: 1.038\n",
      "Epoch: 52, Train Loss: 1.134, Validation Loss: 1.028\n",
      "Epoch: 53, Train Loss: 1.128, Validation Loss: 1.019\n",
      "Epoch: 54, Train Loss: 1.128, Validation Loss: 1.011\n",
      "Epoch: 55, Train Loss: 1.120, Validation Loss: 1.002\n",
      "Epoch: 56, Train Loss: 1.101, Validation Loss: 0.995\n",
      "Epoch: 57, Train Loss: 1.097, Validation Loss: 0.987\n",
      "Epoch: 58, Train Loss: 1.087, Validation Loss: 0.979\n",
      "Epoch: 59, Train Loss: 1.088, Validation Loss: 0.971\n",
      "Epoch: 60, Train Loss: 1.065, Validation Loss: 0.964\n",
      "Epoch: 61, Train Loss: 1.075, Validation Loss: 0.957\n",
      "Epoch: 62, Train Loss: 1.055, Validation Loss: 0.950\n",
      "Epoch: 63, Train Loss: 1.047, Validation Loss: 0.943\n",
      "Epoch: 64, Train Loss: 1.046, Validation Loss: 0.936\n",
      "Epoch: 65, Train Loss: 1.045, Validation Loss: 0.930\n",
      "Epoch: 66, Train Loss: 1.033, Validation Loss: 0.923\n",
      "Epoch: 67, Train Loss: 1.029, Validation Loss: 0.917\n",
      "Epoch: 68, Train Loss: 1.025, Validation Loss: 0.911\n",
      "Epoch: 69, Train Loss: 1.017, Validation Loss: 0.906\n",
      "Epoch: 70, Train Loss: 1.002, Validation Loss: 0.900\n",
      "Epoch: 71, Train Loss: 0.995, Validation Loss: 0.894\n",
      "Epoch: 72, Train Loss: 0.997, Validation Loss: 0.888\n",
      "Epoch: 73, Train Loss: 0.996, Validation Loss: 0.884\n",
      "Epoch: 74, Train Loss: 0.986, Validation Loss: 0.878\n",
      "Epoch: 75, Train Loss: 0.987, Validation Loss: 0.873\n",
      "Epoch: 76, Train Loss: 0.969, Validation Loss: 0.868\n",
      "Epoch: 77, Train Loss: 0.972, Validation Loss: 0.863\n",
      "Epoch: 78, Train Loss: 0.965, Validation Loss: 0.858\n",
      "Epoch: 79, Train Loss: 0.970, Validation Loss: 0.853\n",
      "Epoch: 80, Train Loss: 0.958, Validation Loss: 0.849\n",
      "Epoch: 81, Train Loss: 0.948, Validation Loss: 0.844\n",
      "Epoch: 82, Train Loss: 0.943, Validation Loss: 0.840\n",
      "Epoch: 83, Train Loss: 0.941, Validation Loss: 0.836\n",
      "Epoch: 84, Train Loss: 0.955, Validation Loss: 0.831\n",
      "Epoch: 85, Train Loss: 0.937, Validation Loss: 0.827\n",
      "Epoch: 86, Train Loss: 0.923, Validation Loss: 0.823\n",
      "Epoch: 87, Train Loss: 0.924, Validation Loss: 0.819\n",
      "Epoch: 88, Train Loss: 0.924, Validation Loss: 0.815\n",
      "Epoch: 89, Train Loss: 0.928, Validation Loss: 0.811\n",
      "Epoch: 90, Train Loss: 0.917, Validation Loss: 0.807\n",
      "Epoch: 91, Train Loss: 0.902, Validation Loss: 0.803\n",
      "Epoch: 92, Train Loss: 0.901, Validation Loss: 0.800\n",
      "Epoch: 93, Train Loss: 0.897, Validation Loss: 0.796\n",
      "Epoch: 94, Train Loss: 0.887, Validation Loss: 0.792\n",
      "Epoch: 95, Train Loss: 0.892, Validation Loss: 0.789\n",
      "Epoch: 96, Train Loss: 0.879, Validation Loss: 0.785\n",
      "Epoch: 97, Train Loss: 0.889, Validation Loss: 0.781\n",
      "Epoch: 98, Train Loss: 0.893, Validation Loss: 0.778\n",
      "Epoch: 99, Train Loss: 0.874, Validation Loss: 0.775\n",
      "Epoch: 100, Train Loss: 0.870, Validation Loss: 0.772\n",
      "Epoch: 101, Train Loss: 0.877, Validation Loss: 0.769\n",
      "Epoch: 102, Train Loss: 0.876, Validation Loss: 0.765\n",
      "Epoch: 103, Train Loss: 0.862, Validation Loss: 0.762\n",
      "Epoch: 104, Train Loss: 0.858, Validation Loss: 0.759\n",
      "Epoch: 105, Train Loss: 0.856, Validation Loss: 0.756\n",
      "Epoch: 106, Train Loss: 0.859, Validation Loss: 0.753\n",
      "Epoch: 107, Train Loss: 0.860, Validation Loss: 0.750\n",
      "Epoch: 108, Train Loss: 0.844, Validation Loss: 0.747\n",
      "Epoch: 109, Train Loss: 0.852, Validation Loss: 0.744\n",
      "Epoch: 110, Train Loss: 0.841, Validation Loss: 0.741\n",
      "Epoch: 111, Train Loss: 0.836, Validation Loss: 0.739\n",
      "Epoch: 112, Train Loss: 0.828, Validation Loss: 0.736\n",
      "Epoch: 113, Train Loss: 0.833, Validation Loss: 0.734\n",
      "Epoch: 114, Train Loss: 0.834, Validation Loss: 0.731\n",
      "Epoch: 115, Train Loss: 0.822, Validation Loss: 0.728\n",
      "Epoch: 116, Train Loss: 0.824, Validation Loss: 0.725\n",
      "Epoch: 117, Train Loss: 0.827, Validation Loss: 0.723\n",
      "Epoch: 118, Train Loss: 0.817, Validation Loss: 0.720\n",
      "Epoch: 119, Train Loss: 0.823, Validation Loss: 0.718\n",
      "Epoch: 120, Train Loss: 0.807, Validation Loss: 0.715\n",
      "Epoch: 121, Train Loss: 0.808, Validation Loss: 0.713\n",
      "Epoch: 122, Train Loss: 0.799, Validation Loss: 0.710\n",
      "Epoch: 123, Train Loss: 0.808, Validation Loss: 0.708\n",
      "Epoch: 124, Train Loss: 0.808, Validation Loss: 0.705\n",
      "Epoch: 125, Train Loss: 0.805, Validation Loss: 0.703\n",
      "Epoch: 126, Train Loss: 0.797, Validation Loss: 0.701\n",
      "Epoch: 127, Train Loss: 0.797, Validation Loss: 0.699\n",
      "Epoch: 128, Train Loss: 0.796, Validation Loss: 0.696\n",
      "Epoch: 129, Train Loss: 0.791, Validation Loss: 0.694\n",
      "Epoch: 130, Train Loss: 0.795, Validation Loss: 0.692\n",
      "Epoch: 131, Train Loss: 0.786, Validation Loss: 0.690\n",
      "Epoch: 132, Train Loss: 0.788, Validation Loss: 0.688\n",
      "Epoch: 133, Train Loss: 0.775, Validation Loss: 0.685\n",
      "Epoch: 134, Train Loss: 0.781, Validation Loss: 0.684\n",
      "Epoch: 135, Train Loss: 0.775, Validation Loss: 0.681\n",
      "Epoch: 136, Train Loss: 0.775, Validation Loss: 0.679\n",
      "Epoch: 137, Train Loss: 0.774, Validation Loss: 0.677\n",
      "Epoch: 138, Train Loss: 0.775, Validation Loss: 0.675\n",
      "Epoch: 139, Train Loss: 0.776, Validation Loss: 0.673\n",
      "Epoch: 140, Train Loss: 0.776, Validation Loss: 0.671\n",
      "Epoch: 141, Train Loss: 0.757, Validation Loss: 0.670\n",
      "Epoch: 142, Train Loss: 0.775, Validation Loss: 0.668\n",
      "Epoch: 143, Train Loss: 0.769, Validation Loss: 0.666\n",
      "Epoch: 144, Train Loss: 0.763, Validation Loss: 0.664\n",
      "Epoch: 145, Train Loss: 0.762, Validation Loss: 0.662\n",
      "Epoch: 146, Train Loss: 0.745, Validation Loss: 0.660\n",
      "Epoch: 147, Train Loss: 0.749, Validation Loss: 0.658\n",
      "Epoch: 148, Train Loss: 0.755, Validation Loss: 0.656\n",
      "Epoch: 149, Train Loss: 0.757, Validation Loss: 0.654\n",
      "Epoch: 150, Train Loss: 0.755, Validation Loss: 0.653\n",
      "Epoch: 151, Train Loss: 0.742, Validation Loss: 0.651\n",
      "Epoch: 152, Train Loss: 0.737, Validation Loss: 0.649\n",
      "Epoch: 153, Train Loss: 0.733, Validation Loss: 0.648\n",
      "Epoch: 154, Train Loss: 0.737, Validation Loss: 0.646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 155, Train Loss: 0.732, Validation Loss: 0.644\n",
      "Epoch: 156, Train Loss: 0.734, Validation Loss: 0.643\n",
      "Epoch: 157, Train Loss: 0.726, Validation Loss: 0.641\n",
      "Epoch: 158, Train Loss: 0.725, Validation Loss: 0.639\n",
      "Epoch: 159, Train Loss: 0.731, Validation Loss: 0.638\n",
      "Epoch: 160, Train Loss: 0.729, Validation Loss: 0.636\n",
      "Epoch: 161, Train Loss: 0.732, Validation Loss: 0.634\n",
      "Epoch: 162, Train Loss: 0.731, Validation Loss: 0.633\n",
      "Epoch: 163, Train Loss: 0.724, Validation Loss: 0.631\n",
      "Epoch: 164, Train Loss: 0.731, Validation Loss: 0.630\n",
      "Epoch: 165, Train Loss: 0.721, Validation Loss: 0.628\n",
      "Epoch: 166, Train Loss: 0.709, Validation Loss: 0.627\n",
      "Epoch: 167, Train Loss: 0.714, Validation Loss: 0.625\n",
      "Epoch: 168, Train Loss: 0.712, Validation Loss: 0.623\n",
      "Epoch: 169, Train Loss: 0.709, Validation Loss: 0.622\n",
      "Epoch: 170, Train Loss: 0.700, Validation Loss: 0.621\n",
      "Epoch: 171, Train Loss: 0.712, Validation Loss: 0.619\n",
      "Epoch: 172, Train Loss: 0.703, Validation Loss: 0.618\n",
      "Epoch: 173, Train Loss: 0.717, Validation Loss: 0.616\n",
      "Epoch: 174, Train Loss: 0.707, Validation Loss: 0.615\n",
      "Epoch: 175, Train Loss: 0.711, Validation Loss: 0.614\n",
      "Epoch: 176, Train Loss: 0.706, Validation Loss: 0.612\n",
      "Epoch: 177, Train Loss: 0.703, Validation Loss: 0.611\n",
      "Epoch: 178, Train Loss: 0.697, Validation Loss: 0.610\n",
      "Epoch: 179, Train Loss: 0.705, Validation Loss: 0.608\n",
      "Epoch: 180, Train Loss: 0.696, Validation Loss: 0.607\n",
      "Epoch: 181, Train Loss: 0.706, Validation Loss: 0.605\n",
      "Epoch: 182, Train Loss: 0.694, Validation Loss: 0.604\n",
      "Epoch: 183, Train Loss: 0.702, Validation Loss: 0.603\n",
      "Epoch: 184, Train Loss: 0.694, Validation Loss: 0.601\n",
      "Epoch: 185, Train Loss: 0.695, Validation Loss: 0.600\n",
      "Epoch: 186, Train Loss: 0.691, Validation Loss: 0.599\n",
      "Epoch: 187, Train Loss: 0.683, Validation Loss: 0.598\n",
      "Epoch: 188, Train Loss: 0.688, Validation Loss: 0.596\n",
      "Epoch: 189, Train Loss: 0.684, Validation Loss: 0.595\n",
      "Epoch: 190, Train Loss: 0.676, Validation Loss: 0.594\n",
      "Epoch: 191, Train Loss: 0.693, Validation Loss: 0.593\n",
      "Epoch: 192, Train Loss: 0.669, Validation Loss: 0.591\n",
      "Epoch: 193, Train Loss: 0.676, Validation Loss: 0.590\n",
      "Epoch: 194, Train Loss: 0.668, Validation Loss: 0.589\n",
      "Epoch: 195, Train Loss: 0.673, Validation Loss: 0.588\n",
      "Epoch: 196, Train Loss: 0.674, Validation Loss: 0.587\n",
      "Epoch: 197, Train Loss: 0.673, Validation Loss: 0.585\n",
      "Epoch: 198, Train Loss: 0.669, Validation Loss: 0.584\n",
      "Epoch: 199, Train Loss: 0.678, Validation Loss: 0.583\n",
      "Epoch: 200, Train Loss: 0.665, Validation Loss: 0.582\n",
      "Epoch: 201, Train Loss: 0.665, Validation Loss: 0.581\n",
      "Epoch: 202, Train Loss: 0.665, Validation Loss: 0.579\n",
      "Epoch: 203, Train Loss: 0.659, Validation Loss: 0.578\n",
      "Epoch: 204, Train Loss: 0.662, Validation Loss: 0.578\n",
      "Epoch: 205, Train Loss: 0.661, Validation Loss: 0.576\n",
      "Epoch: 206, Train Loss: 0.670, Validation Loss: 0.575\n",
      "Epoch: 207, Train Loss: 0.668, Validation Loss: 0.574\n",
      "Epoch: 208, Train Loss: 0.660, Validation Loss: 0.573\n",
      "Epoch: 209, Train Loss: 0.663, Validation Loss: 0.572\n",
      "Epoch: 210, Train Loss: 0.660, Validation Loss: 0.571\n",
      "Epoch: 211, Train Loss: 0.660, Validation Loss: 0.570\n",
      "Epoch: 212, Train Loss: 0.653, Validation Loss: 0.569\n",
      "Epoch: 213, Train Loss: 0.655, Validation Loss: 0.568\n",
      "Epoch: 214, Train Loss: 0.650, Validation Loss: 0.567\n",
      "Epoch: 215, Train Loss: 0.650, Validation Loss: 0.566\n",
      "Epoch: 216, Train Loss: 0.648, Validation Loss: 0.565\n",
      "Epoch: 217, Train Loss: 0.643, Validation Loss: 0.564\n",
      "Epoch: 218, Train Loss: 0.647, Validation Loss: 0.563\n",
      "Epoch: 219, Train Loss: 0.644, Validation Loss: 0.562\n",
      "Epoch: 220, Train Loss: 0.648, Validation Loss: 0.561\n",
      "Epoch: 221, Train Loss: 0.640, Validation Loss: 0.560\n",
      "Epoch: 222, Train Loss: 0.639, Validation Loss: 0.559\n",
      "Epoch: 223, Train Loss: 0.633, Validation Loss: 0.558\n",
      "Epoch: 224, Train Loss: 0.633, Validation Loss: 0.557\n",
      "Epoch: 225, Train Loss: 0.638, Validation Loss: 0.556\n",
      "Epoch: 226, Train Loss: 0.634, Validation Loss: 0.555\n",
      "Epoch: 227, Train Loss: 0.631, Validation Loss: 0.554\n",
      "Epoch: 228, Train Loss: 0.633, Validation Loss: 0.553\n",
      "Epoch: 229, Train Loss: 0.636, Validation Loss: 0.552\n",
      "Epoch: 230, Train Loss: 0.632, Validation Loss: 0.551\n",
      "Epoch: 231, Train Loss: 0.627, Validation Loss: 0.550\n",
      "Epoch: 232, Train Loss: 0.632, Validation Loss: 0.549\n",
      "Epoch: 233, Train Loss: 0.634, Validation Loss: 0.549\n",
      "Epoch: 234, Train Loss: 0.625, Validation Loss: 0.548\n",
      "Epoch: 235, Train Loss: 0.629, Validation Loss: 0.547\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "for i in range(epochs):\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    for j in range(steps_per_epoch):\n",
    "        X_batch, y_batch = resample(X_train, y_train, n_samples=batch_size)\n",
    "        X.value, y.value = X_batch, y_batch\n",
    "        \n",
    "        forward_and_backward(graph)\n",
    "        sgd_update(trainables, lr)\n",
    "        train_loss += graph[-1].value\n",
    "        \n",
    "    train_loss = train_loss / steps_per_epoch\n",
    "    losses['train'].append(train_loss)\n",
    "    \n",
    "    X.value, y.value = X_validation, y_validation\n",
    "    forward_and_backward(graph, training=False)\n",
    "    val_loss = graph[-1].value\n",
    "    losses['validation'].append(val_loss)\n",
    "    \n",
    "    #if (i+1) % batch_size == 0:\n",
    "    print(\"Epoch: {}, Train Loss: {:.3f}, Validation Loss: {:.3f}\".format(i+1, train_loss, val_loss))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(len(losses['train']))\n",
    "ax.plot(x, losses['train'], label='Train')\n",
    "ax.plot(x, losses['validation'], label='Validation')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "feed_dict[X] = X_test\n",
    "feed_dict[y] = y_test\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "forward_and_backward(graph, training=False)\n",
    "loss = graph[-1].value\n",
    "print(\"Test loss: {:.3f}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
